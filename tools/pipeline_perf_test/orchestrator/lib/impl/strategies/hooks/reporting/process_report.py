from typing import Optional, List, ClassVar, Dict, Any

import pandas as pd
import yaml
from pydantic import Field, BaseModel

from .....core.context.base import BaseContext
from .....core.context import FrameworkElementHookContext
from .....runner.schema.reporting_hook_config import StandardReportingHookStrategyConfig
from .....core.framework.report import Report, ReportAggregation
from ...common.events import get_start_end_event
from .....runner.registry import hook_registry, PluginMeta, ReportMeta
from .....core.telemetry.telemetry_client import TelemetryClient
from .....core.helpers.report import group_by_populated_columns
from .....core.helpers.metrics import (
    format_metrics_by_ordered_rules,
    format_bytes,
    append_string,
    compute_rate_over_time,
    delta,
    pivot_aggregated_metrics,
    concat_metrics_df,
    split_raw_metrics_by_group,
)
from .standard_reporting_strategy import StandardReportingStrategy


STRATEGY_NAME = "process_report"


class ProcessReportIncludesConfig(BaseModel):
    """
    Configuration for selecting which report sections to include in the process report.

    Attributes:
        component_summary (bool, optional): Whether to include a high-level summary
            of component-level execution status and metadata. Defaults to True.

        component_detail (bool, optional): Whether to include detailed, per-step
            execution breakdowns for each component. Defaults to False.

    This model is typically nested under a parent reporting config to allow fine-grained
    control over the structure of a process-level report.
    """

    component_summary: Optional[bool] = True
    component_detail: Optional[bool] = False


@hook_registry.register_config(STRATEGY_NAME)
class ProcessReportHookConfig(StandardReportingHookStrategyConfig):
    """
    Configuration model for the process report hook strategy.

    This configuration extends the base `StandardReportingHookStrategyConfig` and
    provides additional control over which components to include and how to structure
    the resulting process-level report.

    Attributes:
        components (List[str], optional): A list of component names to include in
            the report. If None, all components in the suite will be considered.

        output_component_summary (bool, optional): Whether to include a summary of
            all component statuses in the top-level report output. Defaults to True.

        include_sections (ProcessReportIncludesConfig, optional): Nested configuration
            specifying which report sections (summary, detail, etc.) to include.

    This config is used by the `ProcessReportHook` strategy (or subclasses thereof) to
    determine both the content and scope of the generated report.
    """

    components: Optional[List[str]] = None
    output_component_summary: Optional[bool] = True
    include_sections: Optional[ProcessReportIncludesConfig] = Field(
        default_factory=ProcessReportIncludesConfig
    )


class ProcessReport(Report):
    """
    A specialized report class for summarizing and comparing system resource usage
    (CPU, memory, network) across components during test execution.

    This report is generated by the `ProcessReportHook` strategy and supports both
    per-run summaries and multi-run comparisons, depending on the aggregation mode.

    Attributes:
        config (Optional[ProcessReportHookConfig]): Configuration used to generate the report.
        REPORT_TYPE (str): Report type identifier used for registration and output formatting.

    Class Methods:
        aggregate(reports, mode, label_key):
            Aggregates multiple reports into a comparison structure, suitable for rendering
            summary comparisons across test runs. Each component's metrics are merged
            by metric name and labeled by run.

        to_aggregate_template_dict(results, config, mode):
            Transforms aggregated result data into a dictionary structure suitable for
            rendering via templates. Applies formatting rules based on metric type
            (e.g., bytes, CPU cores, rates).

        get_template(mode):
            Returns the default Jinja-compatible markdown template string used to render
            the report in various aggregation modes (e.g., comparison, timeseries).

    Instance Methods:
        to_template_dict():
            Converts the report into a structured dictionary, applying formatting and
            markdown rendering to key sections (e.g., component summaries). Useful for
            integrating with markdown-based reports or UI tools.

    Supported Sections (configurable):
        - Component Summary: Aggregated view (min/mean/max for gauges, deltas for counters)
        - Component Detail: Raw or rate-transformed time-series metrics per component (optional)

    Formatting Rules:
        Automatically applies unit formatting for metrics like:
        - container.memory.usage -> formatted as human-readable bytes
        - container.network.rx/tx -> formatted as bytes
        - container.cpu.usage -> formatted in cores
        - rate(x) metrics -> suffixed with '/s'

    Example Output (Markdown):
        ```
        # Process Report

        ## Metadata:
        report.observation.start: 2024-01-01T00:00:00Z
        report.observation.end:   2024-01-01T00:05:00Z

        ## Process: load-generator
        | metric_name        | delta | mean | max  |
        |--------------------|-------|------|------|
        | container.cpu.usage| 0.05  | 0.03 | 0.07 |
        ```

    Raises:
        KeyError: if expected metric results are missing in the report results.
        ValueError: if unsupported aggregation mode is specified.
    """

    config: Optional[ProcessReportHookConfig] = None
    REPORT_TYPE: ClassVar[str] = STRATEGY_NAME

    @classmethod
    def aggregate(
        cls,
        reports: List["Report"],
        mode: ReportAggregation = ReportAggregation.COMPARISON,
        *,
        label_key: str = "name",
    ) -> Dict[str, pd.DataFrame]:
        component_summary_tables = [r.results["component_summary"] for r in reports]
        # component_detail_tables = [r.results["component_detail"] for r in reports]
        labels = [
            r.metadata.get(label_key, f"Run {i+1}") for i, r in enumerate(reports)
        ]

        if mode == ReportAggregation.COMPARISON:
            component_summary_merged = {}
            for i, df in enumerate(component_summary_tables):
                tables = split_raw_metrics_by_group(
                    df, "metric_attributes.component_name"
                )
                for component, component_df in tables.items():
                    df_clean = component_df.drop(columns=["timestamp"]).copy()
                    df_clean = (
                        df_clean.set_index("metric_name")
                        .rename(columns={"value": labels[i]})
                        .reset_index()
                    )
                    if component_summary_merged.get(component) is None:
                        component_summary_merged[component] = df_clean
                    else:
                        component_summary_merged[component] = pd.merge(
                            component_summary_merged[component],
                            df_clean,
                            on="metric_name",
                            how="outer",
                        )

            return {
                "component_summary": component_summary_merged,
                # "component_detail": component_detail_merged,
            }

    @classmethod
    def to_aggregate_template_dict(
        cls,
        results: Dict[str, pd.DataFrame],
        config: ProcessReportHookConfig,
        mode: ReportAggregation = ReportAggregation.COMPARISON,
    ) -> Dict[str, Any]:
        data = {}
        format_rules = [
            (r"(?:container|process)\.network\.(?:rx|tx)", format_bytes),
            (r"(?:container|process)\.memory\.usage", format_bytes),
            (r"rate\(", append_string("/s")),
            (r"(?:container|process)\.cpu\.usage", append_string(" cores")),
        ]

        if mode == ReportAggregation.COMPARISON:
            if config.include_sections.component_summary:
                cs = results.get("component_summary")
                data["component_summary"] = {}
                for component, df in cs.items():
                    df = df.copy()
                    df = format_metrics_by_ordered_rules(
                        df, metric_col="metric_name", format_rules=format_rules
                    )
                    df = df.fillna("")
                    data["component_summary"][component] = df.to_markdown(index=False)
        return data

    def to_template_dict(self):
        data = self.to_dict()
        data["metadata"] = yaml.dump(self.metadata, indent=2)

        format_rules = [
            (r"(?:container|process)\.network\.(?:rx|tx)", format_bytes),
            (r"(?:container|process)\.memory\.usage", format_bytes),
            (r"rate\(", append_string("/s")),
            (r"(?:container|process).cpu.usage", append_string(" cores")),
        ]

        if self.config.output_component_summary:
            data["component_summary"] = {}
            component_summary = self.results.get("component_summary")
            component_summary_pivoted = pivot_aggregated_metrics(
                component_summary, group_key="metric_attributes.component_name"
            )
            for key, df in component_summary_pivoted.items():
                df = df.copy()
                try:
                    df = group_by_populated_columns(df, ["delta", "min", "max", "mean"])
                except ValueError:
                    df = group_by_populated_columns(df, ["min", "max", "mean"])

                df = format_metrics_by_ordered_rules(
                    df, metric_col="metric_name", format_rules=format_rules
                )
                df = df.fillna("")
                data["component_summary"][key] = df.to_markdown(index=False)
        return data

    @classmethod
    def get_template(cls, mode: ReportAggregation) -> str:
        default = """# Process Report

## Metadata:

{{ report.metadata }}
{% if report.component_summary %}
{% for process, table in report.component_summary.items() %}
## Process: {{ process }}

{{table}}
{% endfor %}

{% endif %}
"""
        templates = {
            ReportAggregation.NONE: default,
            ReportAggregation.COMPARISON: default.replace(
                "Process Report", "Process Comparison Report"
            ),
            ReportAggregation.TIMESERIES: default,
        }
        return templates.get(mode, "Unsupported aggregation mode")


@hook_registry.register_class(STRATEGY_NAME)
class ProcessReportHook(StandardReportingStrategy):
    """
    Reporting strategy that generates a process-level metrics report across components.

    This hook collects telemetry metrics (CPU, memory, network) for all or a subset
    of components in the test suite and builds a structured report summarizing
    system resource usage during execution.

    The report includes:
        - Component-level summaries (min/mean/max for gauges; delta for counters)
        - Optional per-component detailed time series data
        - Optional observation window start/end times and total duration (if configured)

    Attributes:
        config (ProcessReportHookConfig): Configuration specifying what to include,
            how to filter components, and whether to restrict the report to a
            specific time window using start/end events.
        report_start (datetime or None): Timestamp for the beginning of the observation
            window, inferred from telemetry events.
        report_end (datetime or None): Timestamp for the end of the observation window.
        duration (float or None): Duration of the observation window in seconds.

    Methods:
        _execute(ctx: BaseContext) -> Report:
            Internal method to collect telemetry data, transform it, and populate
            a structured `ProcessReport` instance with both raw and aggregated metrics.

    Raises:
        RuntimeError: If `ctx` does not conform to expected context structure or
            if telemetry data cannot be resolved as expected.
    """

    PLUGIN_META = PluginMeta(
        supported_contexts=[FrameworkElementHookContext.__name__],
        installs_hooks=[],
        yaml_example="""
hooks:
  run:
    post:
    - process_report:
        name: resource_observation
        between_events: ["observation_start", "observation_stop"]
        include_sections:
          component_summary: true
          component_detail: false
""",
        report_meta=ReportMeta(
            supported_aggregations=[
                ReportAggregation.COMPARISON.value,
                ReportAggregation.NONE.value,
            ],
            sample_output={
                "Without Aggregation": """
# Process Report

## Metadata:

test.suite: Test OTLP Vs OTAP
...

## Process: otel-collector

| metric_name                | delta     | max        | mean       | min        |
|:---------------------------|:----------|:-----------|:-----------|:-----------|
| container.network.rx       | 115.77 MB |            |            |            |
| container.network.tx       | 473.57 MB |            |            |            |
| container.cpu.usage        |           | 3.43 cores | 3.35 cores | 3.29 cores |
| container.memory.usage     |           | 222.38 MB  | 213.00 MB  | 200.57 MB  |
| rate(container.network.rx) |           | 14.77 MB/s | 14.54 MB/s | 14.08 MB/s |
| rate(container.network.tx) |           | 60.57 MB/s | 59.47 MB/s | 57.77 MB/s |
""",
                "Comparison Aggregation": """
# Process Comparison Report

## Metadata:

test.suite: Test OTLP Vs OTAP
...

## Process: otel-collector

| metric_name                      | Process OTLP   | Process OTAP   |
|:---------------------------------|:---------------|:---------------|
| delta(container.network.rx)      | 434.61 MB      | 115.77 MB      |
| delta(container.network.tx)      | 434.11 MB      | 473.57 MB      |
| max(container.cpu.usage)         | 4.05 cores     | 3.43 cores     |
| max(container.memory.usage)      | 128.87 MB      | 222.38 MB      |
| max(rate(container.network.rx))  | 59.04 MB/s     | 14.77 MB/s     |
| max(rate(container.network.tx))  | 58.78 MB/s     | 60.57 MB/s     |
| mean(container.cpu.usage)        | 3.82 cores     | 3.35 cores     |
| mean(container.memory.usage)     | 117.46 MB      | 213.00 MB      |
| mean(rate(container.network.rx)) | 53.88 MB/s     | 14.54 MB/s     |
| mean(rate(container.network.tx)) | 53.81 MB/s     | 59.47 MB/s     |
| min(container.cpu.usage)         | 3.25 cores     | 3.29 cores     |
| min(container.memory.usage)      | 111.25 MB      | 200.57 MB      |
| min(rate(container.network.rx))  | 48.50 MB/s     | 14.08 MB/s     |
| min(rate(container.network.tx))  | 48.25 MB/s     | 57.77 MB/s     |
""",
            },
        ),
    )

    def __init__(self, config: ProcessReportHookConfig):
        super().__init__(config)
        self.config = config
        self.report_start = None
        self.report_end = None
        self.duration = None

    def _execute(self, ctx: BaseContext) -> Report:
        tc: TelemetryClient = ctx.get_telemetry_client()
        components = ctx.get_components()
        logger = ctx.get_logger(__name__)

        report = ProcessReport.from_context(self.config.name, ctx)
        report.config = self.config

        if self.config.between_events:
            (report_start_event, report_end_event) = get_start_end_event(
                self.config.between_events, tc
            )
            self.report_start = report_start_event.iloc[0]["timestamp"]
            report.metadata["report.observation.start"] = (
                pd.to_datetime(self.report_start).isoformat()
                if self.report_start
                else None
            )
            self.report_end = report_end_event.iloc[0]["timestamp"]
            report.metadata["report.observation.end"] = (
                pd.to_datetime(self.report_end).isoformat() if self.report_end else None
            )

            if self.report_start and self.report_end:
                self.duration = (
                    pd.to_datetime(self.report_end) - pd.to_datetime(self.report_start)
                ).total_seconds()
                report.metadata["report.observation.duration_seconds"] = self.duration

        results = {}
        for component_name, component in components.items():
            if self.config.components and component_name not in self.config.components:
                continue

            metric_prefix = "container"
            if component.deployment.type == "process":
                metric_prefix = "process"

            process_counter_metrics = tc.metrics.query_metrics(
                metric_name=[
                    f"{metric_prefix}.network.rx",
                    f"{metric_prefix}.network.tx",
                ],
                metric_attrs={"component_name": component_name},
                time_range=(self.report_start, self.report_end),
            )
            process_gauge_metrics = tc.metrics.query_metrics(
                metric_name=[
                    f"{metric_prefix}.cpu.usage",
                    f"{metric_prefix}.memory.usage",
                ],
                metric_attrs={"component_name": component_name},
                time_range=(self.report_start, self.report_end),
            )

            counter_rates = pd.DataFrame()
            counter_deltas = pd.DataFrame()
            # Check if the process_counter_metrics DataFrame is empty
            if process_counter_metrics.empty:
                if metric_prefix == "process":
                    logger.debug(
                        "Component is a process deployment, so no network metrics expected: %s",
                        component_name,
                    )
                else:
                    logger.warning(
                        "Missing process counter metrics for component %s",
                        component_name,
                    )
                    continue
            else:
                counter_rates = compute_rate_over_time(
                    process_counter_metrics,
                    by=["metric_attributes.component_name", "metric_name"],
                )
                counter_deltas = process_counter_metrics.with_aggregation(
                    by=["metric_attributes.component_name", "metric_name"],
                    agg_func=delta,
                )

            # Check if the process_gauge_metrics DataFrame is empty
            if process_gauge_metrics.empty:
                logger.warning(
                    "Missing process gauge metrics for component %s", component_name
                )
                continue

            # Component detail
            gauge_metrics = concat_metrics_df(
                [counter_rates, process_gauge_metrics], ignore_index=True
            )
            if results.get("component_detail") is None:
                results["component_detail"] = concat_metrics_df(
                    [counter_rates, gauge_metrics], ignore_index=True
                )
            else:
                results["component_detail"] = concat_metrics_df(
                    [results.get("component_detail"), counter_rates, gauge_metrics],
                    ignore_index=True,
                )

            # Component Summary
            gauge_aggregates = gauge_metrics.with_aggregation(
                by=["metric_attributes.component_name", "metric_name"],
                agg_func=["min", "mean", "max"],
            )
            if results.get("component_summary") is None:
                results["component_summary"] = concat_metrics_df(
                    [gauge_aggregates, counter_deltas], ignore_index=True
                )
            else:
                results["component_summary"] = concat_metrics_df(
                    [
                        results.get("component_summary"),
                        gauge_aggregates,
                        counter_deltas,
                    ],
                    ignore_index=True,
                )

        report.set_results(results)
        return report
