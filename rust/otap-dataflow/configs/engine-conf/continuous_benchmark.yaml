version: otel_dataflow/v1

# This configuration file reproduces the continuous benchmarking setup used
# in our CI pipelines. The traffic generators, system under test, and backend
# are all included in a single configuration for easier local testing/debugging.
#
# Runtime CLI overrides:
# - --num-cores / --core-id-range override top-level
#   `policies.resources.core_allocation`.
# - Pipeline/group-level `policies.resources` still take precedence over that
#   top-level value.
# - --http-admin-bind overrides `engine.http_admin.bind_address`.
#
# If you want --num-cores / --core-id-range to drive all pipelines uniformly,
# remove the pipeline-level `policies.resources` sections below.

# Top-level policy.
# Values below match the engine defaults (explicit to showcase the v1 policy model).
policies:
  channel_capacity:
    control:
      node: 256
      pipeline: 256
    pdata: 128
  health: {}
  telemetry:
    pipeline_metrics: true
    tokio_metrics: true
    channel_metrics: true
  resources:
    core_allocation:
      type: all_cores

# Engine-wide settings.
engine:
  http_admin:
    bind_address: 127.0.0.1:8085
  telemetry:
    logs:
      level: info

  # Internal telemetry system (ITS) declaration.
  observability:
    pipeline:
      nodes:
        itr:
          type: internal_telemetry:receiver
          config: {}
        sink:
          type: noop:exporter
          config: null
      connections:
        - from: itr
          to: sink

# Pipeline groups are used to logically separate sets of pipelines.
# Resolution order for regular pipelines is:
# pipeline.policies -> group.policies -> top-level policies
# (replacement is per policy family, not deep-merge).
groups:
  continuous_benchmark:
    # Group-level policies are optional. This one is explicit and matches
    # defaults, to demonstrate the hierarchy without changing behavior.
    policies:
      channel_capacity:
        control:
          node: 256
          pipeline: 256
        pdata: 128

    pipelines:
      # ======================================================================
      # Traffic generation pipelines
      # ======================================================================

      # First traffic generator: static pre-generated dataset.
      # Pipeline-level resources override group/top-level resources.
      traffic_gen1:
        policies:
          resources:
            core_allocation:
              type: core_count
              count: 15

        nodes:
          receiver:
            type: traffic_generator:receiver
            config:
              data_source: static
              generation_strategy: pre_generated
              traffic_config:
                signals_per_second: 150000
                max_signal_count: null
                metric_weight: 0
                trace_weight: 0
                log_weight: 30
          exporter:
            type: otlp:exporter
            config:
              grpc_endpoint: "http://127.0.0.1:4327"

        connections:
          - from: receiver
            to: exporter

      # Second traffic generator: dynamic generation from semantic conventions.
      traffic_gen2:
        policies:
          resources:
            core_allocation:
              type: core_set
              set:
                - start: 21
                  end: 35

        nodes:
          receiver:
            type: traffic_generator:receiver
            config:
              traffic_config:
                signals_per_second: 100000
                max_signal_count: null
                metric_weight: 0
                trace_weight: 0
                log_weight: 30
              registry_path: https://github.com/open-telemetry/semantic-conventions.git[model]
          exporter:
            type: otlp:exporter
            config:
              grpc_endpoint: "http://127.0.0.1:4337"

        connections:
          - from: receiver
            to: exporter

      # ======================================================================
      # System Under Test pipeline
      # ======================================================================
      sut:
        policies:
          resources:
            core_allocation:
              type: core_set
              set:
                - start: 0
                  end: 1

        nodes:
          otlp_recv1:
            type: otlp:receiver
            config:
              protocols:
                grpc:
                  listening_addr: "127.0.0.1:4327"
                  wait_for_result: true
          otlp_recv2:
            type: otlp:receiver
            config:
              protocols:
                grpc:
                  listening_addr: "127.0.0.1:4337"
                  wait_for_result: true

          router:
            type: type_router:processor
            outputs: ["logs", "metrics", "traces"]
            config: {}

          retry:
            type: retry:processor
            config:
              multiplier: 1.5

          logs_exporter:
            type: otlp:exporter
            config:
              grpc_endpoint: "http://127.0.0.1:4328"
              max_in_flight: 6

          metrics_exporter:
            type: noop:exporter
            config: null

          spans_exporter:
            type: noop:exporter
            config: null

        connections:
          - from: otlp_recv1
            to: router
          - from: otlp_recv2
            to: router
          - from: router["logs"]
            to: retry
          - from: router["metrics"]
            to: metrics_exporter
          - from: router["traces"]
            to: spans_exporter
          - from: retry
            to: logs_exporter

      # ======================================================================
      # Backend pipeline
      # ======================================================================
      backend:
        policies:
          resources:
            core_allocation:
              type: core_set
              set:
                - start: 1
                  end: 1

        nodes:
          receiver:
            type: otlp:receiver
            config:
              protocols:
                grpc:
                  listening_addr: 127.0.0.1:4328

          perf_noop:
            type: noop:exporter
            config: null

        connections:
          - from: receiver
            to: perf_noop
