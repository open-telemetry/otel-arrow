# Shows the batch processor.
settings:
  default_pipeline_ctrl_msg_channel_size: 100
  default_node_ctrl_msg_channel_size: 100
  default_pdata_channel_size: 100

nodes:
  receiver:
    kind: receiver
    plugin_urn: "urn:otel:otap:fake_data_generator:receiver"
    out_ports:
      out_port:
        destinations:
          - batch
        dispatch_strategy: round_robin
    config:
      traffic_config:
        max_signal_count: 10000
        max_batch_size: 100
        signals_per_second: 100
        log_weight: 100
      registry_path: https://github.com/open-telemetry/semantic-conventions.git[model]
  batch:
    kind: processor
    plugin_urn: "urn:otel:batch:processor"
    out_ports:
      out_port:
        destinations:
          - debug
        dispatch_strategy: round_robin
    config:
      # At the rate configured above, this will print a batch
      # every 10 seconds.
      send_batch_size: 1000
      # This interrupts it, so we expect around 300.
      timeout: 3s
  debug:
    kind: processor
    plugin_urn: "urn:otel:debug:processor"
    out_ports:
      out_port:
        destinations:
          - noop
        dispatch_strategy: round_robin
    config:
      verbosity: detailed
      mode: signal
  noop:
    kind: exporter
    plugin_urn: "urn:otel:noop:exporter"
    config:
