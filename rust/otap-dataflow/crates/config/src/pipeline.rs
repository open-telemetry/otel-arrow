// Copyright The OpenTelemetry Authors
// SPDX-License-Identifier: Apache-2.0

//! Pipeline configuration specification.

pub mod service;

use crate::error::{Context, Error, HyperEdgeSpecDetails};
use crate::health::HealthPolicy;
use crate::node::{DispatchStrategy, HyperEdgeConfig, NodeKind, NodeUserConfig};
use crate::observed_state::ObservedStateSettings;
use crate::pipeline::service::ServiceConfig;
use crate::pipeline::service::telemetry::logs::INTERNAL_TELEMETRY_RECEIVER_URN;
use crate::{Description, NodeId, NodeUrn, PipelineGroupId, PipelineId, PortName};
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use serde_json::Value;
use std::collections::{HashMap, HashSet};
use std::path::Path;
use std::sync::Arc;

/// A pipeline configuration describing the interconnections between nodes.
/// A pipeline is a directed acyclic graph that could be qualified as a hyper-DAG:
/// - "Hyper" because the edges connecting the nodes can be hyper-edges.  
/// - A node can be connected to multiple outgoing nodes.  
/// - The way messages are dispatched over each hyper-edge is defined by a dispatch strategy representing
///   different communication model semantics. For example, it could be a broadcast channel that sends
///   the same message to all destination nodes, or it might have a round-robin or least-loaded semantic,
///   similar to an SPMC channel.
///
/// This configuration defines the pipeline’s nodes, the interconnections (hyper-edges), and pipeline-level settings.
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(deny_unknown_fields)]
pub struct PipelineConfig {
    /// Type of the pipeline, which determines the type of PData it processes.
    ///
    /// Note: Even though technically our engine can support several types of pdata, we
    /// are focusing our efforts on the OTAP pipeline (hence the default value).
    #[serde(default = "default_pipeline_type")]
    r#type: PipelineType,

    /// Settings for this pipeline.
    #[serde(default)]
    settings: PipelineSettings,

    /// All nodes in this pipeline.
    ///
    /// Note: We use `Arc<NodeUserConfig>` to allow sharing the same pipeline configuration
    /// across multiple cores/threads without cloning the entire configuration.
    #[serde(default)]
    nodes: PipelineNodes,

    /// Internal telemetry pipeline nodes.
    ///
    /// This optional section defines nodes for processing internal telemetry
    /// (logs, metrics, traces generated by the engine itself).
    ///
    /// The internal pipeline runs on a dedicated thread with hardcoded settings
    /// (single thread, no admin server), separate from the main pipeline.
    ///
    /// Constraints:
    /// - Receivers must be Internal Telemetry Receivers (ITR)
    ///   with plugin_urn matching `INTERNAL_TELEMETRY_RECEIVER_URN`
    /// - Processors and exporters can be any valid plugin
    #[serde(default, skip_serializing_if = "PipelineNodes::is_empty")]
    internal: PipelineNodes,

    /// Service-level telemetry configuration.
    #[serde(default)]
    service: ServiceConfig,
}

fn default_pipeline_type() -> PipelineType {
    PipelineType::Otap
}

/// The type of pipeline, which can be either OTLP (OpenTelemetry Protocol) or
/// OTAP (OpenTelemetry with Apache Arrow Protocol).
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "snake_case")]
pub enum PipelineType {
    /// OpenTelemetry Protocol (OTLP) pipeline.
    /// ToDo: With the recent benchmark results on proto_bytes->views->OTAP, we could consider to get rid of the OTLP pipeline type.
    Otlp,
    /// OpenTelemetry with Apache Arrow Protocol (OTAP) pipeline.
    Otap,
}

/// A collection of nodes forming a pipeline graph (hyper-DAG).
///
/// This wrapper provides validation methods for the node graph structure,
/// including hyper-edge validation and cycle detection.
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema, Default)]
#[serde(transparent)]
pub struct PipelineNodes(HashMap<NodeId, Arc<NodeUserConfig>>);

impl PipelineNodes {
    /// Returns true if the node collection is empty.
    #[must_use]
    pub fn is_empty(&self) -> bool {
        self.0.is_empty()
    }

    /// Returns the number of nodes.
    #[must_use]
    pub fn len(&self) -> usize {
        self.0.len()
    }

    /// Returns a reference to the node with the given ID, if it exists.
    #[must_use]
    pub fn get(&self, id: &str) -> Option<&Arc<NodeUserConfig>> {
        self.0.get(id)
    }

    /// Returns true if a node with the given ID exists.
    #[must_use]
    pub fn contains_key(&self, id: &str) -> bool {
        self.0.contains_key(id)
    }

    /// Returns an iterator visiting all nodes.
    pub fn iter(&self) -> impl Iterator<Item = (&NodeId, &Arc<NodeUserConfig>)> {
        self.0.iter()
    }

    /// Returns an iterator over node IDs.
    pub fn keys(&self) -> impl Iterator<Item = &NodeId> {
        self.0.keys()
    }

    /// Validate the node graph structure.
    ///
    /// Checks for:
    /// - Invalid hyper-edges (missing target nodes)
    /// - Cycles in the DAG
    pub fn validate(
        &self,
        pipeline_group_id: &PipelineGroupId,
        pipeline_id: &PipelineId,
        errors: &mut Vec<Error>,
    ) {
        self.validate_hyper_edges(pipeline_group_id, pipeline_id, errors);

        // Only check for cycles if no hyper-edge errors
        if errors.is_empty() {
            for cycle in self.detect_cycles() {
                errors.push(Error::CycleDetected {
                    context: Context::new(pipeline_group_id.clone(), pipeline_id.clone()),
                    nodes: cycle,
                });
            }
        }
    }

    /// Validate hyper-edges (check that all destination nodes exist).
    fn validate_hyper_edges(
        &self,
        pipeline_group_id: &PipelineGroupId,
        pipeline_id: &PipelineId,
        errors: &mut Vec<Error>,
    ) {
        for (node_id, node) in self.0.iter() {
            for edge in node.out_ports.values() {
                let missing_targets: Vec<_> = edge
                    .destinations
                    .iter()
                    .filter(|target| !self.0.contains_key(*target))
                    .cloned()
                    .collect();

                if !missing_targets.is_empty() {
                    errors.push(Error::InvalidHyperEdgeSpec {
                        context: Context::new(pipeline_group_id.clone(), pipeline_id.clone()),
                        source_node: node_id.clone(),
                        missing_source: false,
                        details: Box::new(HyperEdgeSpecDetails {
                            target_nodes: edge.destinations.iter().cloned().collect(),
                            dispatch_strategy: edge.dispatch_strategy.clone(),
                            missing_targets,
                        }),
                    });
                }
            }
        }
    }

    /// Detect cycles in the node graph.
    fn detect_cycles(&self) -> Vec<Vec<NodeId>> {
        fn visit(
            node: &NodeId,
            nodes: &HashMap<NodeId, Arc<NodeUserConfig>>,
            visiting: &mut HashSet<NodeId>,
            visited: &mut HashSet<NodeId>,
            current_path: &mut Vec<NodeId>,
            cycles: &mut Vec<Vec<NodeId>>,
        ) {
            if visited.contains(node) {
                return;
            }
            if visiting.contains(node) {
                if let Some(pos) = current_path.iter().position(|n| n == node) {
                    cycles.push(current_path[pos..].to_vec());
                }
                return;
            }
            _ = visiting.insert(node.clone());
            current_path.push(node.clone());

            if let Some(n) = nodes.get(node) {
                for edge in n.out_ports.values() {
                    for tgt in &edge.destinations {
                        visit(tgt, nodes, visiting, visited, current_path, cycles);
                    }
                }
            }

            _ = visiting.remove(node);
            _ = visited.insert(node.clone());
            _ = current_path.pop();
        }

        let mut visiting = HashSet::new();
        let mut current_path = Vec::new();
        let mut visited = HashSet::new();
        let mut cycles = Vec::new();

        for node in self.0.keys() {
            if !visited.contains(node) {
                visit(
                    node,
                    &self.0,
                    &mut visiting,
                    &mut visited,
                    &mut current_path,
                    &mut cycles,
                );
            }
        }

        cycles
    }
}

impl std::ops::Index<&str> for PipelineNodes {
    type Output = Arc<NodeUserConfig>;

    fn index(&self, id: &str) -> &Self::Output {
        &self.0[id]
    }
}

impl IntoIterator for PipelineNodes {
    type Item = (NodeId, Arc<NodeUserConfig>);
    type IntoIter = std::collections::hash_map::IntoIter<NodeId, Arc<NodeUserConfig>>;

    fn into_iter(self) -> Self::IntoIter {
        self.0.into_iter()
    }
}

impl FromIterator<(NodeId, Arc<NodeUserConfig>)> for PipelineNodes {
    fn from_iter<T: IntoIterator<Item = (NodeId, Arc<NodeUserConfig>)>>(iter: T) -> Self {
        Self(iter.into_iter().collect())
    }
}

/// A configuration for a pipeline.
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
pub struct PipelineSettings {
    /// The default size of the node control message channels.
    /// These channels are used for sending control messages by the pipeline engine to nodes.
    #[serde(default = "default_node_ctrl_msg_channel_size")]
    pub default_node_ctrl_msg_channel_size: usize,

    /// The default size of the pipeline control message channels.
    /// This MPSC channel is used for sending control messages from nodes to the pipeline engine.
    #[serde(default = "default_pipeline_ctrl_msg_channel_size")]
    pub default_pipeline_ctrl_msg_channel_size: usize,

    /// The default size of the pdata channels.
    #[serde(default = "default_pdata_channel_size")]
    pub default_pdata_channel_size: usize,

    /// Observed state settings.
    ///
    /// TODO: consider this internal logging configuration?
    #[serde(default)]
    pub observed_state: ObservedStateSettings,

    /// Health policy.
    #[serde(default)]
    pub health_policy: HealthPolicy,

    /// Pipeline-level telemetry settings.
    ///
    /// These flags control capture of pipeline runtime metrics emitted by the pipeline
    /// execution loop (e.g. per-pipeline CPU/memory metrics and Tokio runtime metrics).
    ///
    /// This is distinct from `service.telemetry`, which configures exporting of OpenTelemetry
    /// signals to external backends.
    ///
    /// TODO: fix the overlap with service::telemetry! This has only
    /// metric configuration That has OpenTelemetry-declarative mixed
    /// with this PR's LoggingProviders{3xProviderMode} and OutputMode
    /// choics.
    #[serde(default)]
    pub telemetry: TelemetrySettings,
}

/// Configuration for pipeline-internal telemetry capture.
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
pub struct TelemetrySettings {
    /// Enable capture of per-pipeline internal metrics.
    ///
    /// When disabled, the engine does not update or report the `pipeline.metrics` metric set.
    #[serde(default = "default_true")]
    pub pipeline_metrics: bool,

    /// Enable capture of Tokio runtime internal metrics.
    ///
    /// When disabled, the engine does not update or report the `tokio.runtime.metrics` metric set.
    #[serde(default = "default_true")]
    pub tokio_metrics: bool,

    /// Enable capture of channel-level metrics.
    ///
    /// When disabled, the engine does not report channel sender/receiver metrics.
    #[serde(default = "default_true")]
    pub channel_metrics: bool,
}

const fn default_true() -> bool {
    true
}

impl Default for TelemetrySettings {
    fn default() -> Self {
        Self {
            pipeline_metrics: true,
            tokio_metrics: true,
            channel_metrics: true,
        }
    }
}

fn default_node_ctrl_msg_channel_size() -> usize {
    100
}
fn default_pipeline_ctrl_msg_channel_size() -> usize {
    100
}
fn default_pdata_channel_size() -> usize {
    100
}

impl Default for PipelineSettings {
    fn default() -> Self {
        Self {
            default_node_ctrl_msg_channel_size: default_node_ctrl_msg_channel_size(),
            default_pipeline_ctrl_msg_channel_size: default_pipeline_ctrl_msg_channel_size(),
            default_pdata_channel_size: default_pdata_channel_size(),
            observed_state: ObservedStateSettings::default(),
            health_policy: HealthPolicy::default(),
            telemetry: TelemetrySettings::default(),
        }
    }
}

impl PipelineConfig {
    /// Create a new [`PipelineConfig`] from a JSON string.
    pub fn from_json(
        pipeline_group_id: PipelineGroupId,
        pipeline_id: PipelineId,
        json_str: &str,
    ) -> Result<Self, Error> {
        let cfg: PipelineConfig =
            serde_json::from_str(json_str).map_err(|e| Error::DeserializationError {
                context: Context::new(pipeline_group_id.clone(), pipeline_id.clone()),
                format: "JSON".to_string(),
                details: e.to_string(),
            })?;

        cfg.validate(&pipeline_group_id, &pipeline_id)?;
        Ok(cfg)
    }

    /// Create a new [`PipelineConfig`] from a YAML string.
    pub fn from_yaml(
        pipeline_group_id: PipelineGroupId,
        pipeline_id: PipelineId,
        yaml_str: &str,
    ) -> Result<Self, Error> {
        let spec: PipelineConfig =
            serde_yaml::from_str(yaml_str).map_err(|e| Error::DeserializationError {
                context: Context::new(pipeline_group_id.clone(), pipeline_id.clone()),
                format: "YAML".to_string(),
                details: e.to_string(),
            })?;

        spec.validate(&pipeline_group_id, &pipeline_id)?;
        Ok(spec)
    }

    /// Load a [`PipelineConfig`] from a JSON file.
    pub fn from_json_file<P: AsRef<Path>>(
        pipeline_group_id: PipelineGroupId,
        pipeline_id: PipelineId,
        path: P,
    ) -> Result<Self, Error> {
        let contents = std::fs::read_to_string(path).map_err(|e| Error::FileReadError {
            context: Context::new(pipeline_group_id.clone(), pipeline_id.clone()),
            details: e.to_string(),
        })?;
        Self::from_json(pipeline_group_id, pipeline_id, &contents)
    }

    /// Load a [`PipelineConfig`] from a YAML file.
    pub fn from_yaml_file<P: AsRef<Path>>(
        pipeline_group_id: PipelineGroupId,
        pipeline_id: PipelineId,
        path: P,
    ) -> Result<Self, Error> {
        let contents = std::fs::read_to_string(path).map_err(|e| Error::FileReadError {
            context: Context::new(pipeline_group_id.clone(), pipeline_id.clone()),
            details: e.to_string(),
        })?;
        Self::from_yaml(pipeline_group_id, pipeline_id, &contents)
    }

    /// Load a [`PipelineConfig`] from a file, automatically detecting the format based on file extension.
    ///
    /// Supports:
    /// - JSON files: `.json`
    /// - YAML files: `.yaml`, `.yml`
    pub fn from_file<P: AsRef<Path>>(
        pipeline_group_id: PipelineGroupId,
        pipeline_id: PipelineId,
        path: P,
    ) -> Result<Self, Error> {
        let path = path.as_ref();
        let extension = path
            .extension()
            .and_then(|ext| ext.to_str())
            .map(|ext| ext.to_lowercase());

        match extension.as_deref() {
            Some("json") => Self::from_json_file(pipeline_group_id, pipeline_id, path),
            Some("yaml") | Some("yml") => {
                Self::from_yaml_file(pipeline_group_id, pipeline_id, path)
            }
            _ => {
                let context = Context::new(pipeline_group_id, pipeline_id);
                let details = format!(
                    "Unsupported file extension: {}. Supported extensions are: .json, .yaml, .yml",
                    extension.unwrap_or_else(|| "<none>".to_string())
                );
                Err(Error::FileReadError { context, details })
            }
        }
    }

    /// Returns the general settings for this pipeline.
    #[must_use]
    pub fn pipeline_settings(&self) -> &PipelineSettings {
        &self.settings
    }

    /// Returns a reference to the main pipeline nodes.
    #[must_use]
    pub fn nodes(&self) -> &PipelineNodes {
        &self.nodes
    }

    /// Returns an iterator visiting all nodes in the pipeline.
    pub fn node_iter(&self) -> impl Iterator<Item = (&NodeId, &Arc<NodeUserConfig>)> {
        self.nodes.iter()
    }

    /// Creates a consuming iterator over the nodes in the pipeline.
    pub fn node_into_iter(self) -> impl Iterator<Item = (NodeId, Arc<NodeUserConfig>)> {
        self.nodes.into_iter()
    }

    /// Returns the service-level telemetry configuration.
    #[must_use]
    pub fn service(&self) -> &ServiceConfig {
        &self.service
    }

    /// Returns true if the internal telemetry pipeline is configured.
    #[must_use]
    pub fn has_internal_pipeline(&self) -> bool {
        !self.internal.is_empty()
    }

    /// Returns a reference to the internal pipeline nodes.
    #[must_use]
    pub fn internal_nodes(&self) -> &PipelineNodes {
        &self.internal
    }

    /// Returns an iterator visiting all nodes in the internal telemetry pipeline.
    pub fn internal_node_iter(&self) -> impl Iterator<Item = (&NodeId, &Arc<NodeUserConfig>)> {
        self.internal.iter()
    }

    /// Extracts the internal pipeline as a separate, independent `PipelineConfig`.
    ///
    /// This creates a complete pipeline configuration from the internal nodes,
    /// with hardcoded settings appropriate for internal telemetry processing.
    #[must_use]
    pub fn extract_internal_config(&self) -> Option<PipelineConfig> {
        if self.internal.is_empty() {
            return None;
        }

        Some(PipelineConfig {
            r#type: self.r#type.clone(),
            settings: Self::internal_pipeline_settings(),
            nodes: self.internal.clone(),
            internal: PipelineNodes::default(),
            service: ServiceConfig::default(),
        })
    }

    /// Returns hardcoded settings for the internal telemetry pipeline.
    ///
    /// TODO: these are hard-coded, add configurability.
    #[must_use]
    pub fn internal_pipeline_settings() -> PipelineSettings {
        PipelineSettings {
            default_node_ctrl_msg_channel_size: 50,
            default_pipeline_ctrl_msg_channel_size: 50,
            default_pdata_channel_size: 50,
            observed_state: ObservedStateSettings::default(),
            health_policy: HealthPolicy::default(),
            telemetry: TelemetrySettings {
                pipeline_metrics: false,
                tokio_metrics: false,
                channel_metrics: false,
            },
        }
    }

    /// Validate the pipeline specification.
    ///
    /// This method checks for:
    /// - Invalid hyper-edges (missing target nodes)
    /// - Cycles in the DAG
    /// - Internal pipeline receivers are only ITR nodes
    pub fn validate(
        &self,
        pipeline_group_id: &PipelineGroupId,
        pipeline_id: &PipelineId,
    ) -> Result<(), Error> {
        let mut errors = Vec::new();

        // Validate main pipeline
        self.nodes
            .validate(pipeline_group_id, pipeline_id, &mut errors);

        // Validate internal pipeline if present
        if !self.internal.is_empty() {
            // Check that receivers in internal pipeline are only ITR nodes
            for (node_id, node) in self.internal.iter() {
                if node.kind == NodeKind::Receiver
                    && node.plugin_urn.as_ref() != INTERNAL_TELEMETRY_RECEIVER_URN
                {
                    errors.push(Error::InvalidInternalReceiver {
                        context: Context::new(pipeline_group_id.clone(), pipeline_id.clone()),
                        node_id: node_id.clone(),
                        plugin_urn: node.plugin_urn.to_string(),
                    });
                }
            }

            // Validate internal pipeline graph structure
            self.internal
                .validate(pipeline_group_id, pipeline_id, &mut errors);
        }

        if !errors.is_empty() {
            Err(Error::InvalidConfiguration { errors })
        } else {
            Ok(())
        }
    }
}

/// A builder for constructing a [`PipelineConfig`].
pub struct PipelineConfigBuilder {
    description: Option<Description>,
    nodes: HashMap<NodeId, NodeUserConfig>,
    duplicate_nodes: Vec<NodeId>,
    pending_connections: Vec<PendingConnection>,
}

struct PendingConnection {
    src: NodeId,
    out_port: PortName,
    targets: HashSet<NodeId>,
    strategy: DispatchStrategy,
}

impl PipelineConfigBuilder {
    /// Create a new pipeline builder.
    #[must_use]
    pub fn new() -> Self {
        Self {
            description: None,
            nodes: HashMap::new(),
            duplicate_nodes: Vec::new(),
            pending_connections: Vec::new(),
        }
    }

    /// Set the description of the pipeline.
    #[must_use]
    pub fn description(mut self, description: Description) -> Self {
        self.description = Some(description);
        self
    }

    /// Add a node with a given id and kind.
    /// Optionally provide config.
    pub fn add_node<S: Into<NodeId>, U: Into<NodeUrn>>(
        mut self,
        id: S,
        kind: NodeKind,
        plugin_urn: U,
        config: Option<Value>,
    ) -> Self {
        let id = id.into();
        let plugin_urn = plugin_urn.into();
        if self.nodes.contains_key(&id) {
            self.duplicate_nodes.push(id.clone());
        } else {
            _ = self.nodes.insert(
                id.clone(),
                NodeUserConfig {
                    kind,
                    plugin_urn,
                    description: None,
                    out_ports: HashMap::new(),
                    default_out_port: None,
                    config: config.unwrap_or(Value::Null),
                },
            );
        }
        self
    }

    /// Add a receiver node.
    pub fn add_receiver<S: Into<NodeId>, U: Into<NodeUrn>>(
        self,
        id: S,
        plugin_urn: U,
        config: Option<Value>,
    ) -> Self {
        self.add_node(id, NodeKind::Receiver, plugin_urn, config)
    }

    /// Add a processor node.
    pub fn add_processor<S: Into<NodeId>, U: Into<NodeUrn>>(
        self,
        id: S,
        plugin_urn: U,
        config: Option<Value>,
    ) -> Self {
        self.add_node(id, NodeKind::Processor, plugin_urn, config)
    }

    /// Add an exporter node.
    pub fn add_exporter<S: Into<NodeId>, U: Into<NodeUrn>>(
        self,
        id: S,
        plugin_urn: U,
        config: Option<Value>,
    ) -> Self {
        self.add_node(id, NodeKind::Exporter, plugin_urn, config)
    }

    /// Connect source node's out_port to one or more target nodes
    /// with a given dispatch strategy.
    pub fn connect<S, P, T, I>(
        mut self,
        src: S,
        out_port: P,
        targets: I,
        strategy: DispatchStrategy,
    ) -> Self
    where
        S: Into<NodeId>,
        P: Into<PortName>,
        T: Into<NodeId>,
        I: IntoIterator<Item = T>,
    {
        self.pending_connections.push(PendingConnection {
            src: src.into(),
            out_port: out_port.into(),
            targets: targets.into_iter().map(Into::into).collect(),
            strategy,
        });
        self
    }

    /// Connect source node's out_port to one or more target nodes
    /// with a round-robin dispatch strategy.
    pub fn broadcast<S, P, T, I>(self, src: S, out_port: P, targets: I) -> Self
    where
        S: Into<NodeId>,
        P: Into<PortName>,
        T: Into<NodeId>,
        I: IntoIterator<Item = T>,
    {
        self.connect(src, out_port, targets, DispatchStrategy::Broadcast)
    }

    /// Connect source node's out_port to one or more target nodes
    /// with a round-robin dispatch strategy.
    pub fn round_robin<S, P, T, I>(self, src: S, out_port: P, targets: I) -> Self
    where
        S: Into<NodeId>,
        P: Into<PortName>,
        T: Into<NodeId>,
        I: IntoIterator<Item = T>,
    {
        self.connect(src, out_port, targets, DispatchStrategy::RoundRobin)
    }

    /// Connect source node's out_port to one or more target nodes
    /// with a random dispatch strategy.
    pub fn random<S, P, T, I>(self, src: S, out_port: P, targets: I) -> Self
    where
        S: Into<NodeId>,
        P: Into<PortName>,
        T: Into<NodeId>,
        I: IntoIterator<Item = T>,
    {
        self.connect(src, out_port, targets, DispatchStrategy::Random)
    }

    /// Connect source node's out_port to one or more target nodes
    /// with a least-loaded dispatch strategy.
    pub fn least_loaded<S, P, T, I>(self, src: S, out_port: P, targets: I) -> Self
    where
        S: Into<NodeId>,
        P: Into<PortName>,
        T: Into<NodeId>,
        I: IntoIterator<Item = T>,
    {
        self.connect(src, out_port, targets, DispatchStrategy::LeastLoaded)
    }

    /// Validate and build the pipeline specification.
    ///
    /// We collect all possible errors (duplicate nodes, duplicate out-ports,
    /// missing source/targets, invalid edges, cycles) into one `InvalidHyperDag`
    /// report. This lets callers see every problem at once, rather than failing
    /// fast on the first error.
    pub fn build<T, P>(
        mut self,
        pipeline_type: PipelineType,
        pipeline_group_id: T,
        pipeline_id: P,
    ) -> Result<PipelineConfig, Error>
    where
        T: Into<PipelineGroupId>,
        P: Into<PipelineId>,
    {
        let mut errors = Vec::new();
        let pipeline_group_id = pipeline_group_id.into();
        let pipeline_id = pipeline_id.into();

        // Report duplicated nodes
        for node_id in &self.duplicate_nodes {
            errors.push(Error::DuplicateNode {
                context: Context::new(pipeline_group_id.clone(), pipeline_id.clone()),
                node_id: node_id.clone(),
            });
        }

        // Detect duplicate out‐ports (same src + port used twice)
        {
            let mut seen_ports = HashSet::new();
            for conn in &self.pending_connections {
                let key = (conn.src.clone(), conn.out_port.clone());
                if !seen_ports.insert(key.clone()) {
                    errors.push(Error::DuplicateOutPort {
                        context: Context::new(pipeline_group_id.clone(), pipeline_id.clone()),
                        source_node: conn.src.clone(),
                        port: conn.out_port.clone(),
                    });
                }
            }
        }

        // Process each pending connection (skipping any 2nd+ duplicates)
        let mut inserted_ports = HashSet::new();
        for conn in self.pending_connections {
            let key = (conn.src.clone(), conn.out_port.clone());
            if !inserted_ports.insert(key.clone()) {
                // skip this duplicate
                continue;
            }

            // check that source & all targets exist
            let mut missing = Vec::new();
            let src_exists = self.nodes.contains_key(&conn.src);
            for t in &conn.targets {
                if !self.nodes.contains_key(t) {
                    missing.push(t.clone());
                }
            }

            // if anything is missing, record as InvalidHyperEdgeSpec
            if !src_exists || !missing.is_empty() {
                errors.push(Error::InvalidHyperEdgeSpec {
                    context: Context::new(pipeline_group_id.clone(), pipeline_id.clone()),
                    source_node: conn.src.clone(),
                    missing_source: !src_exists,
                    details: Box::new(HyperEdgeSpecDetails {
                        target_nodes: conn.targets.iter().cloned().collect(),
                        dispatch_strategy: conn.strategy,
                        missing_targets: missing,
                    }),
                });
                continue;
            }

            // finally, insert into the node’s out_ports
            if let Some(node) = self.nodes.get_mut(&conn.src) {
                let _ = node.out_ports.insert(
                    conn.out_port.clone(),
                    HyperEdgeConfig {
                        destinations: conn.targets.clone(),
                        dispatch_strategy: conn.strategy,
                    },
                );
            }
        }

        if !errors.is_empty() {
            Err(Error::InvalidConfiguration { errors })
        } else {
            // Build the spec and validate it
            let spec = PipelineConfig {
                nodes: self
                    .nodes
                    .into_iter()
                    .map(|(id, node)| (id, Arc::new(node)))
                    .collect(),
                internal: PipelineNodes::default(),
                settings: PipelineSettings::default(),
                r#type: pipeline_type,
                service: ServiceConfig::default(),
            };

            spec.validate(&pipeline_group_id, &pipeline_id)?;
            Ok(spec)
        }
    }
}

impl Default for PipelineConfigBuilder {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use crate::error::Error;
    use crate::node::DispatchStrategy;
    use crate::pipeline::service::telemetry::metrics::MetricsConfig;
    use crate::pipeline::service::telemetry::metrics::readers::periodic::MetricsPeriodicExporterConfig;
    use crate::pipeline::service::telemetry::metrics::readers::{
        MetricsReaderConfig, MetricsReaderPeriodicConfig,
    };
    use crate::pipeline::service::telemetry::{AttributeValue, TelemetryConfig};
    use crate::pipeline::{PipelineConfigBuilder, PipelineType};
    use serde_json::json;

    #[test]
    fn test_duplicate_node_errors() {
        let result = PipelineConfigBuilder::new()
            .add_receiver("A", "urn:test:receiver", None)
            .add_processor("A", "urn:test:processor", None) // duplicate
            .build(PipelineType::Otap, "pgroup", "pipeline");

        match result {
            Err(Error::InvalidConfiguration { errors }) => {
                // Should only report one DuplicateNode
                assert_eq!(errors.len(), 1);
                match &errors[0] {
                    Error::DuplicateNode { node_id, .. } if node_id == "A" => {}
                    other => panic!("expected DuplicateNode(\"A\"), got {other:?}"),
                }
            }
            other => panic!("expected Err(InvalidPipelineSpec), got {other:?}"),
        }
    }

    #[test]
    fn test_duplicate_outport_errors() {
        let result = PipelineConfigBuilder::new()
            .add_receiver("A", "urn:test:receiver", None)
            .add_exporter("B", "urn:test:exporter", None)
            .round_robin("A", "p", ["B"])
            .round_robin("A", "p", ["B"]) // duplicate port on A
            .build(PipelineType::Otap, "pgroup", "pipeline");

        match result {
            Err(Error::InvalidConfiguration { errors }) => {
                // One DuplicateOutPort, no InvalidHyperEdge, no cycles
                assert_eq!(errors.len(), 1);
                match &errors[0] {
                    Error::DuplicateOutPort {
                        source_node, port, ..
                    } if source_node == "A" && port == "p" => {}
                    other => panic!("expected DuplicateOutPort(A, p), got {other:?}"),
                }
            }
            other => panic!("expected Err(InvalidPipelineSpec), got {other:?}"),
        }
    }

    #[test]
    fn test_missing_source_error() {
        let result = PipelineConfigBuilder::new()
            .add_receiver("B", "urn:test:receiver", None)
            .connect("X", "out", ["B"], DispatchStrategy::Broadcast) // X does not exist
            .build(PipelineType::Otap, "pgroup", "pipeline");

        match result {
            Err(Error::InvalidConfiguration { errors }) => {
                assert_eq!(errors.len(), 1);
                match &errors[0] {
                    Error::InvalidHyperEdgeSpec {
                        source_node,
                        missing_source,
                        details,
                        ..
                    } if source_node == "X"
                        && *missing_source
                        && details.missing_targets.is_empty() => {}
                    other => panic!("expected InvalidHyperEdge missing_source, got {other:?}"),
                }
            }
            other => panic!("expected Err(InvalidPipelineSpec), got {other:?}"),
        }
    }

    #[test]
    fn test_missing_target_error() {
        let result = PipelineConfigBuilder::new()
            .add_receiver("A", "urn:test:receiver", None)
            .connect("A", "out", ["Y"], DispatchStrategy::Broadcast) // Y does not exist
            .build(PipelineType::Otap, "pgroup", "pipeline");

        match result {
            Err(Error::InvalidConfiguration { errors }) => {
                assert_eq!(errors.len(), 1);
                match &errors[0] {
                    Error::InvalidHyperEdgeSpec {
                        source_node,
                        missing_source,
                        details,
                        ..
                    } if source_node == "A"
                        && !*missing_source
                        && details.missing_targets.as_slice() == ["Y"]
                        && details.target_nodes.as_slice() == ["Y"] => {}
                    other => panic!("expected InvalidHyperEdge missing_targets, got {other:?}"),
                }
            }
            other => panic!("expected Err(InvalidPipelineSpec), got {other:?}"),
        }
    }

    #[test]
    fn test_cycle_detection_error() {
        let result = PipelineConfigBuilder::new()
            .add_processor("A", "urn:test:processor", None)
            .add_processor("B", "urn:test:processor", None)
            .round_robin("A", "p", ["B"])
            .round_robin("B", "p", ["A"])
            .build(PipelineType::Otap, "pgroup", "pipeline");

        match result {
            Err(Error::InvalidConfiguration { errors }) => {
                // exactly one cycle error
                let mut found = false;
                for err in errors {
                    if let Error::CycleDetected { nodes, .. } = err {
                        // cycle should include A and B
                        assert!(nodes.contains(&"A".into()));
                        assert!(nodes.contains(&"B".into()));
                        found = true;
                    }
                }
                assert!(found, "expected a CycleDetected error");
            }
            other => panic!("expected Err(InvalidPipelineSpec), got {other:?}"),
        }
    }

    #[test]
    fn test_successful_simple_build() {
        let dag = PipelineConfigBuilder::new()
            .add_receiver("Start", "urn:test:receiver", Some(json!({"foo": 1})))
            .add_exporter("End", "urn:test:exporter", None)
            .broadcast("Start", "out", ["End"])
            .build(PipelineType::Otap, "pgroup", "pipeline");

        match dag {
            Ok(pipeline_spec) => {
                // two nodes, one edge on Start
                assert_eq!(pipeline_spec.nodes.len(), 2);
                let start = &pipeline_spec.nodes["Start"];
                assert_eq!(start.out_ports.len(), 1);
                let edge = &start.out_ports["out"];
                assert!(edge.destinations.contains("End"));
            }
            Err(e) => panic!("expected successful build, got {e:?}"),
        }
    }

    #[test]
    fn test_valid_complex_pipeline_spec() {
        let dag = PipelineConfigBuilder::new()
            // ----- TRACES pipeline -----
            .add_receiver(
                "receiver_otlp_traces",
                "urn:test:receiver",
                Some(json!({"desc": "OTLP trace receiver"})),
            )
            .add_processor(
                "processor_batch_traces",
                "urn:test:processor",
                Some(json!({"name": "batch_traces"})),
            )
            .add_processor(
                "processor_resource_traces",
                "urn:test:processor",
                Some(json!({"name": "resource_traces"})),
            )
            .add_processor(
                "processor_traces_to_metrics",
                "urn:test:processor",
                Some(json!({"desc": "convert traces to metrics"})),
            )
            .add_exporter(
                "exporter_otlp_traces",
                "urn:test:exporter",
                Some(json!({"desc": "OTLP trace exporter"})),
            )
            .round_robin("receiver_otlp_traces", "out", ["processor_batch_traces"])
            .round_robin(
                "processor_batch_traces",
                "out",
                ["processor_resource_traces"],
            )
            .round_robin(
                "processor_resource_traces",
                "out",
                ["processor_traces_to_metrics"],
            )
            .round_robin(
                "processor_resource_traces",
                "out2",
                ["exporter_otlp_traces"],
            )
            // ----- METRICS pipeline -----
            .add_receiver(
                "receiver_otlp_metrics",
                "urn:test:receiver",
                Some(json!({"desc": "OTLP metric receiver"})),
            )
            .add_processor(
                "processor_batch_metrics",
                "urn:test:processor",
                Some(json!({"name": "batch_metrics"})),
            )
            .add_processor(
                "processor_metrics_to_events",
                "urn:test:processor",
                Some(json!({"desc": "convert metrics to events"})),
            )
            .add_exporter(
                "exporter_prometheus",
                "urn:test:exporter",
                Some(json!({"desc": "Prometheus exporter"})),
            )
            .add_exporter(
                "exporter_otlp_metrics",
                "urn:test:exporter",
                Some(json!({"desc": "OTLP metric exporter"})),
            )
            .round_robin("receiver_otlp_metrics", "out", ["processor_batch_metrics"])
            .round_robin(
                "processor_batch_metrics",
                "out",
                ["processor_metrics_to_events"],
            )
            .round_robin("processor_batch_metrics", "out2", ["exporter_prometheus"])
            .round_robin("processor_batch_metrics", "out3", ["exporter_otlp_metrics"])
            .round_robin(
                "processor_traces_to_metrics",
                "out",
                ["processor_batch_metrics"],
            )
            // ----- LOGS pipeline -----
            .add_receiver(
                "receiver_filelog",
                "urn:test:receiver",
                Some(json!({"desc": "file log receiver"})),
            )
            .add_receiver(
                "receiver_syslog",
                "urn:test:receiver",
                Some(json!({"desc": "syslog receiver"})),
            )
            .add_processor(
                "processor_filter_logs",
                "urn:test:processor",
                Some(json!({"name": "filter_logs"})),
            )
            .add_processor(
                "processor_logs_to_events",
                "urn:test:processor",
                Some(json!({"desc": "convert logs to events"})),
            )
            .add_exporter(
                "exporter_otlp_logs",
                "urn:test:exporter",
                Some(json!({"desc": "OTLP log exporter"})),
            )
            .round_robin("receiver_filelog", "out", ["processor_filter_logs"])
            .round_robin("receiver_syslog", "out", ["processor_filter_logs"])
            .round_robin("processor_filter_logs", "out", ["processor_logs_to_events"])
            .round_robin("processor_filter_logs", "out2", ["exporter_otlp_logs"])
            // ----- EVENTS pipeline -----
            .add_receiver(
                "receiver_some_events",
                "urn:test:receiver",
                Some(json!({"desc": "custom event receiver"})),
            )
            .add_processor(
                "processor_enrich_events",
                "urn:test:processor",
                Some(json!({"name": "enrich_events"})),
            )
            .add_exporter(
                "exporter_queue_events",
                "urn:test:exporter",
                Some(json!({"desc": "push events to queue"})),
            )
            .round_robin("receiver_some_events", "out", ["processor_enrich_events"])
            .round_robin("processor_enrich_events", "out", ["exporter_queue_events"])
            .round_robin(
                "processor_logs_to_events",
                "out",
                ["processor_enrich_events"],
            )
            .round_robin(
                "processor_metrics_to_events",
                "out",
                ["processor_enrich_events"],
            )
            // Finalize build
            .build(PipelineType::Otap, "pgroup", "pipeline");

        // Assert the DAG is valid and acyclic
        match dag {
            Ok(pipeline_spec) => {
                assert_eq!(pipeline_spec.nodes.len(), 18);
            }
            Err(e) => panic!("Failed to build pipeline DAG: {e:?}"),
        }
    }

    #[test]
    fn test_from_json_file() {
        // Use a dedicated test fixture file
        let file_path = concat!(
            env!("CARGO_MANIFEST_DIR"),
            "/tests/fixtures/test_pipeline.json"
        );

        // Test loading from JSON file
        let result = super::PipelineConfig::from_json_file(
            "test_group".into(),
            "test_pipeline".into(),
            file_path,
        );

        assert!(result.is_ok());
        let config = result.unwrap();
        assert_eq!(config.nodes.len(), 2);
        assert!(config.nodes.contains_key("receiver1"));
        assert!(config.nodes.contains_key("exporter1"));

        let telemetry_config = config.service.telemetry;
        let reporting_interval = telemetry_config.reporting_interval;
        assert_eq!(reporting_interval.as_secs(), 5);

        let resource_attrs = &telemetry_config.resource;

        if let AttributeValue::String(val) = &resource_attrs["service.name"] {
            assert_eq!(val, "test_service");
        } else {
            panic!("Expected service.name to be a string");
        }
        if let AttributeValue::String(val) = &resource_attrs["service.version"] {
            assert_eq!(val, "1.0.0");
        } else {
            panic!("Expected service.version to be a string");
        }
        if let AttributeValue::I64(i) = resource_attrs["instance.id"] {
            assert_eq!(i, 10);
        } else {
            panic!("Expected instance.id to be an integer");
        }

        if let MetricsReaderConfig::Periodic(reader_config) = &telemetry_config.metrics.readers[0] {
            if MetricsPeriodicExporterConfig::Console != reader_config.exporter {
                panic!("Expected MetricsPeriodicExporterConfig");
            }
        } else {
            panic!("Expected first metrics reader to be Periodic");
        }
    }

    #[test]
    fn test_from_yaml_file() {
        // Use a dedicated test fixture file
        let file_path = concat!(
            env!("CARGO_MANIFEST_DIR"),
            "/tests/fixtures/test_pipeline.yaml"
        );

        // Test loading from YAML file
        let result = super::PipelineConfig::from_yaml_file(
            "test_group".into(),
            "test_pipeline".into(),
            file_path,
        );

        assert!(result.is_ok());
        let config = result.unwrap();
        assert_eq!(config.nodes.len(), 3);
        assert!(config.nodes.contains_key("receiver1"));
        assert!(config.nodes.contains_key("processor1"));
        assert!(config.nodes.contains_key("exporter1"));

        let telemetry_config = &config.service().telemetry;
        let reporting_interval = telemetry_config.reporting_interval;
        assert_eq!(reporting_interval.as_secs(), 5);
        let resource_attrs = &telemetry_config.resource;

        if let AttributeValue::String(val) = &resource_attrs["service.name"] {
            assert_eq!(val, "test_service");
        } else {
            panic!("Expected service.name to be a string");
        }
        if let AttributeValue::String(val) = &resource_attrs["service.version"] {
            assert_eq!(val, "1.0.0");
        } else {
            panic!("Expected service.version to be a string");
        }
        if let AttributeValue::I64(i) = resource_attrs["instance.id"] {
            assert_eq!(i, 10);
        } else {
            panic!("Expected instance.id to be an integer");
        }

        if let MetricsReaderConfig::Periodic(reader_config) = &telemetry_config.metrics.readers[0] {
            if MetricsPeriodicExporterConfig::Console != reader_config.exporter {
                panic!("Expected MetricsPeriodicExporterConfig");
            }
        } else {
            panic!("Expected first metrics reader to be Periodic");
        }
    }

    #[test]
    fn test_from_json_file_nonexistent_file() {
        let result = super::PipelineConfig::from_json_file(
            "test_group".into(),
            "test_pipeline".into(),
            "/nonexistent/path/pipeline.json",
        );

        assert!(result.is_err());
        match result {
            Err(Error::FileReadError { .. }) => {}
            other => panic!("Expected FileReadError, got {other:?}"),
        }
    }

    #[test]
    fn test_from_yaml_file_nonexistent_file() {
        let result = super::PipelineConfig::from_yaml_file(
            "test_group".into(),
            "test_pipeline".into(),
            "/nonexistent/path/pipeline.yaml",
        );

        assert!(result.is_err());
        match result {
            Err(Error::FileReadError { .. }) => {}
            other => panic!("Expected FileReadError, got {other:?}"),
        }
    }

    #[test]
    fn test_from_file_json_extension() {
        // Test auto-detection with .json extension
        let file_path = concat!(
            env!("CARGO_MANIFEST_DIR"),
            "/tests/fixtures/test_pipeline.json"
        );

        let result = super::PipelineConfig::from_file(
            "test_group".into(),
            "test_pipeline".into(),
            file_path,
        );

        assert!(result.is_ok());
        let config = result.unwrap();
        assert_eq!(config.nodes.len(), 2);
        assert!(config.nodes.contains_key("receiver1"));
        assert!(config.nodes.contains_key("exporter1"));
    }

    #[test]
    fn test_from_file_yaml_extension() {
        // Test auto-detection with .yaml extension
        let file_path = concat!(
            env!("CARGO_MANIFEST_DIR"),
            "/tests/fixtures/test_pipeline.yaml"
        );

        let result = super::PipelineConfig::from_file(
            "test_group".into(),
            "test_pipeline".into(),
            file_path,
        );

        assert!(result.is_ok());
        let config = result.unwrap();
        assert_eq!(config.nodes.len(), 3);
        assert!(config.nodes.contains_key("receiver1"));
        assert!(config.nodes.contains_key("processor1"));
        assert!(config.nodes.contains_key("exporter1"));
    }

    #[test]
    fn test_from_file_yml_extension() {
        // Test auto-detection with .yml extension (alternative YAML extension)
        // We'll create a simple test using a path that would have .yml extension
        let result = super::PipelineConfig::from_file(
            "test_group".into(),
            "test_pipeline".into(),
            "/nonexistent/test.yml", // This will fail at file reading, but should pass extension detection
        );

        assert!(result.is_err());
        // Should be FileReadError (file doesn't exist), not unsupported extension
        match result {
            Err(Error::FileReadError { details, .. }) => {
                // Make sure it's a file read error and not an extension error
                assert!(!details.contains("Unsupported file extension"));
            }
            other => panic!("Expected FileReadError, got {other:?}"),
        }
    }

    #[test]
    fn test_from_file_unsupported_extension() {
        // Test with unsupported file extension
        let result = super::PipelineConfig::from_file(
            "test_group".into(),
            "test_pipeline".into(),
            "/some/path/config.txt",
        );

        assert!(result.is_err());
        match result {
            Err(Error::FileReadError { details, .. }) => {
                assert!(details.contains("Unsupported file extension"));
                assert!(details.contains("txt"));
                assert!(details.contains(".json, .yaml, .yml"));
            }
            other => panic!("Expected FileReadError with unsupported extension, got {other:?}"),
        }
    }

    #[test]
    fn test_from_file_no_extension() {
        // Test with file that has no extension
        let result = super::PipelineConfig::from_file(
            "test_group".into(),
            "test_pipeline".into(),
            "/some/path/config",
        );

        assert!(result.is_err());
        match result {
            Err(Error::FileReadError { details, .. }) => {
                assert!(details.contains("Unsupported file extension"));
                assert!(details.contains("<none>"));
                assert!(details.contains(".json, .yaml, .yml"));
            }
            other => panic!("Expected FileReadError with no extension, got {other:?}"),
        }
    }

    #[test]
    fn test_telemetry_config_deserialization() {
        let yaml_data: &str = r#"
            reporting_channel_size: 200
            reporting_interval: "5s"
            resource:
              service.name: "my_service"
              service.version: "1.2.3"
            metrics:
              readers:
                - periodic:
                    interval: "15s"
                    exporter:
                      console: {}
            "#;
        let config: TelemetryConfig = serde_yaml::from_str(yaml_data).unwrap();
        assert_eq!(config.reporting_channel_size, 200);
        assert_eq!(config.reporting_interval.as_secs(), 5);

        let resource_attrs = &config.resource;

        if let AttributeValue::String(ref val) = resource_attrs["service.name"] {
            assert_eq!(val, "my_service");
        } else {
            panic!("Expected service.name to be a string");
        }
        if let AttributeValue::String(ref val) = resource_attrs["service.version"] {
            assert_eq!(val, "1.2.3");
        } else {
            panic!("Expected service.version to be a string");
        }

        let readers = &config.metrics.readers;
        assert_eq!(readers.len(), 1);
        if let MetricsReaderConfig::Periodic(periodic_config) = &readers[0] {
            assert_eq!(periodic_config.interval.as_secs(), 15);
            if MetricsPeriodicExporterConfig::Console != periodic_config.exporter {
                panic!("Expected Console exporter config");
            }
        } else {
            panic!("Expected Periodic reader config");
        }
    }

    #[test]
    fn test_metrics_reader_deserialization() {
        let yaml_data = r#"
            readers:
              - periodic:
                  interval: "10s"
                  exporter:
                    console:
            "#;
        let config: MetricsConfig = serde_yaml::from_str(yaml_data).unwrap();
        assert_eq!(config.readers.len(), 1);
        if let MetricsReaderConfig::Periodic(periodic_config) = &config.readers[0] {
            assert_eq!(periodic_config.interval.as_secs(), 10);
            if MetricsPeriodicExporterConfig::Console != periodic_config.exporter {
                panic!("Expected Console exporter config");
            }
        } else {
            panic!("Expected Periodic reader config");
        }
    }

    #[test]
    fn test_metrics_reader_periodic_config_deserialization() {
        let yaml_data = r#"
            interval: "20s"
            exporter:
              console:
            "#;
        let metrics_reader_periodic_config: MetricsReaderPeriodicConfig =
            serde_yaml::from_str(yaml_data).unwrap();
        assert_eq!(metrics_reader_periodic_config.interval.as_secs(), 20);
        if MetricsPeriodicExporterConfig::Console != metrics_reader_periodic_config.exporter {
            panic!("Expected Console exporter config");
        }
    }

    #[test]
    fn test_metrics_reader_periodic_config_deserialization_unknown_exporter() {
        let yaml_data = r#"
            interval: "20s"
            exporter:
              unknown: {}
            "#;
        let metrics_reader_periodic_config_result: Result<
            MetricsReaderPeriodicConfig,
            serde_yaml::Error,
        > = serde_yaml::from_str(yaml_data);
        if let Err(e) = metrics_reader_periodic_config_result {
            let err_msg = e.to_string();
            assert!(err_msg.contains("unknown field `unknown`"));
        } else {
            panic!("Expected deserialization to fail due to unknown exporter");
        }
    }

    #[test]
    fn test_internal_pipeline_with_valid_itr() {
        // Valid internal pipeline with ITR receiver
        let yaml_str = r#"
nodes:
  receiver:
    kind: receiver
    plugin_urn: "urn:test:receiver"
    out_ports:
      out:
        destinations:
          - exporter
        dispatch_strategy: round_robin
  exporter:
    kind: exporter
    plugin_urn: "urn:test:exporter"
    config: {}

internal:
  itr:
    kind: receiver
    plugin_urn: "urn:otel:internal:otlp:receiver"
    out_ports:
      out:
        destinations:
          - internal_exporter
        dispatch_strategy: round_robin
    config: {}
  internal_exporter:
    kind: exporter
    plugin_urn: "urn:test:exporter"
    config: {}
"#;
        let result =
            super::PipelineConfig::from_yaml("test_group".into(), "test_pipeline".into(), yaml_str);
        assert!(result.is_ok());
        let config = result.unwrap();
        assert!(config.has_internal_pipeline());
        assert_eq!(config.internal_node_iter().count(), 2);
    }

    #[test]
    fn test_internal_pipeline_rejects_non_itr_receiver() {
        // Invalid: internal pipeline has a non-ITR receiver
        let yaml_str = r#"
nodes:
  receiver:
    kind: receiver
    plugin_urn: "urn:test:receiver"
    out_ports:
      out:
        destinations:
          - exporter
        dispatch_strategy: round_robin
  exporter:
    kind: exporter
    plugin_urn: "urn:test:exporter"
    config: {}

internal:
  bad_receiver:
    kind: receiver
    plugin_urn: "urn:test:some_other_receiver"
    out_ports:
      out:
        destinations:
          - internal_exporter
        dispatch_strategy: round_robin
    config: {}
  internal_exporter:
    kind: exporter
    plugin_urn: "urn:test:exporter"
    config: {}
"#;
        let result =
            super::PipelineConfig::from_yaml("test_group".into(), "test_pipeline".into(), yaml_str);
        assert!(result.is_err());
        match result {
            Err(Error::InvalidConfiguration { errors }) => {
                assert_eq!(errors.len(), 1);
                match &errors[0] {
                    Error::InvalidInternalReceiver {
                        node_id,
                        plugin_urn,
                        ..
                    } => {
                        assert_eq!(node_id.as_ref(), "bad_receiver");
                        assert_eq!(plugin_urn, "urn:test:some_other_receiver");
                    }
                    other => panic!("Expected InvalidInternalReceiver, got {other:?}"),
                }
            }
            other => panic!("Expected InvalidConfiguration error, got {other:?}"),
        }
    }

    #[test]
    fn test_internal_pipeline_allows_processors_and_exporters() {
        // Valid: internal pipeline can have processors and exporters
        let yaml_str = r#"
nodes:
  receiver:
    kind: receiver
    plugin_urn: "urn:test:receiver"
    out_ports:
      out:
        destinations:
          - exporter
        dispatch_strategy: round_robin
  exporter:
    kind: exporter
    plugin_urn: "urn:test:exporter"
    config: {}

internal:
  itr:
    kind: receiver
    plugin_urn: "urn:otel:internal:otlp:receiver"
    out_ports:
      out:
        destinations:
          - processor
        dispatch_strategy: round_robin
    config: {}
  processor:
    kind: processor
    plugin_urn: "urn:test:processor"
    out_ports:
      out:
        destinations:
          - internal_exporter
        dispatch_strategy: round_robin
    config: {}
  internal_exporter:
    kind: exporter
    plugin_urn: "urn:test:exporter"
    config: {}
"#;
        let result =
            super::PipelineConfig::from_yaml("test_group".into(), "test_pipeline".into(), yaml_str);
        assert!(result.is_ok());
        let config = result.unwrap();
        assert!(config.has_internal_pipeline());
        assert_eq!(config.internal_node_iter().count(), 3);
    }

    #[test]
    fn test_internal_pipeline_validates_hyper_edges() {
        // Invalid: internal pipeline has missing target node
        let yaml_str = r#"
nodes:
  receiver:
    kind: receiver
    plugin_urn: "urn:test:receiver"
    out_ports:
      out:
        destinations:
          - exporter
        dispatch_strategy: round_robin
  exporter:
    kind: exporter
    plugin_urn: "urn:test:exporter"
    config: {}

internal:
  itr:
    kind: receiver
    plugin_urn: "urn:otel:internal:otlp:receiver"
    out_ports:
      out:
        destinations:
          - missing_node
        dispatch_strategy: round_robin
    config: {}
"#;
        let result =
            super::PipelineConfig::from_yaml("test_group".into(), "test_pipeline".into(), yaml_str);
        assert!(result.is_err());
        match result {
            Err(Error::InvalidConfiguration { errors }) => {
                assert_eq!(errors.len(), 1);
                match &errors[0] {
                    Error::InvalidHyperEdgeSpec {
                        source_node,
                        details,
                        ..
                    } => {
                        assert_eq!(source_node.as_ref(), "itr");
                        assert!(details.missing_targets.contains(&"missing_node".into()));
                    }
                    other => panic!("Expected InvalidHyperEdgeSpec, got {other:?}"),
                }
            }
            other => panic!("Expected InvalidConfiguration error, got {other:?}"),
        }
    }

    #[test]
    fn test_empty_internal_pipeline() {
        // Valid: no internal section means no internal pipeline
        let yaml_str = r#"
nodes:
  receiver:
    kind: receiver
    plugin_urn: "urn:test:receiver"
    out_ports:
      out:
        destinations:
          - exporter
        dispatch_strategy: round_robin
  exporter:
    kind: exporter
    plugin_urn: "urn:test:exporter"
    config: {}
"#;
        let result =
            super::PipelineConfig::from_yaml("test_group".into(), "test_pipeline".into(), yaml_str);
        assert!(result.is_ok());
        let config = result.unwrap();
        assert!(!config.has_internal_pipeline());
        assert_eq!(config.internal_node_iter().count(), 0);
    }
}
